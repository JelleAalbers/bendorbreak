
from pathlib import Path
import warnings

import numba
import numpy as np
import paltas
import paltas.Analysis
import pandas as pd
from scipy.optimize import minimize

import bendorbreak as bb
export, __all__ = bb.exporter()



@export
class GaussianInference:
	"""Infer means and (uncorrelated) standard deviations / sigmas
	of a lens population."""

	@classmethod
	def from_folder(cls, folder_path, **kwargs):
		"""Build inference object from folder. Assumes you already did
			run_network_on the folder.

		Arguments:
		 - folder: path to the folder with data

		Other arguments will be passed to GaussianInference.__init__,
		overriding values from the npz if desired.
		"""
		return cls.from_npz(Path(folder_path) / 'network_outputs.npz', **kwargs)

	@classmethod
	def from_npz(cls, npz_path, **kwargs):
		"""Build inference object from an npz generated by run_network_on

		Arguments:
		 - npz_path: path to the npz

		Other arguments will be passed to GaussianInference.__init__,
		overriding values from the npz if desired.
		"""
		with np.load(npz_path, allow_pickle=True) as npz:
			if 'image_prec' not in npz and 'image_cov' in npz:
				npz = dict(npz)
				npz['image_prec'] = np.linalg.inv(npz['image_cov'])

			kwargs_new = dict(
				y_pred=npz['image_mean'][:],
				prec_pred=npz['image_prec'][:],
				all_parameters=npz['param_names'][:],
				population_mean=npz['population_mean'][:],
				population_cov=npz['population_cov'][:],
				training_population_mean=npz['training_population_mean'][:],
				training_population_cov=npz['training_population_cov'][:],
			)
			# Allow overriding of npz values with kwargs
			kwargs_new = {**kwargs_new, **kwargs}
			return cls(**kwargs_new)

	def __init__(
			self,
			y_pred,
			prec_pred=None,
			cov_pred=None,
			population_mean=None,
			population_cov=None,
			test_config_path=None,
			log_sigma=False,
			select_parameters=bb.MARCH_2022_PARAMETERS,
			all_parameters=bb.MARCH_2022_PARAMETERS,
			training_population_mean=None,
			training_population_cov=None,
			n_images=None,
			):
		"""Infer means and (uncorrelated) standard deviations / sigmas
			of a lens population.

		Provide at least the following:
		 - y_pred;
		 - One of prec_pred (preferred) or cov_pred;
		 - One of (population_mean & population_cov) or test_config_path,

		Arguments:
		 - y_pred: (n_images, n_params) array with predicted means
		 - pred_pred: (n_images, n_params, n_params) array with predicted
			inverse covariances
		 - cov_pred: (n_images, n_params, n_params) array with predicted
			covariances
		 - test_config_path: Path to config used to generate the data.
		 	Used to extract population_mean & population_cov if not provided,
			can be omitted otherwise.
		 - population_mean: (n_params) array with population means of params
		 - population_cov: (n_params, n_params) array with population
		 	covariance matrix.
		 - log_sigma: if True, hyperprior will be uniform in Log[sigma]'s.
			Otherwise in hyperprior will be uniform in sigma's.
		 - select_parameters: sequence of strings, parameter names to do
			inference for. Others will be ignored.
		 - all_parameters: sequence of strings, parameter names ordered
			as in y_pred. Defaults to March 2022 paper settings.
		 - training_population_mean: (n_params) array, population_mean
		 	of training set. Defaults to March 2022 paper settings.
		 - training_population_cov: (n_params, n_params) array with
		 	population_cov of training set. Defaults to March 2022 paper.
		 - n_images: if specified, use only the first n_images images.
		"""
		# Get mean/cov of the training data (interim prior)
		if training_population_mean is None:
			training_population_mean, training_population_cov = \
				bb.paltas_mean_cov(bb.DEFAULT_TRAINING_CONFIG, all_parameters)

		# Get true mean/cov of this population (for guess initialization)
		if population_mean is None:
			if test_config_path is None:
				raise ValueError("Provide test_config_path or mean and cov")
			population_mean, population_cov = bb.paltas_mean_cov(test_config_path, all_parameters)

		if prec_pred is None:
			if cov_pred is not None:
				# Recover precision matrices from covariance matrices
				prec_pred = np.linalg.inv(cov_pred)
			else:
				raise ValueError("Provide prec_pred or cov_pred")
		prec_pred = bb.symmetrize_batch(prec_pred)

		# Down-select images (if needed)
		if n_images is not None:
			y_pred = y_pred[:n_images,...]
			prec_pred = prec_pred[:n_images,...]

		# Get indices of parameters to select
		select_is = [
			list(all_parameters).index(p)
			for p in select_parameters]
		# Apply the parameter selection to the vectors/matrices we need
		params = np.asarray(all_parameters)[select_is].tolist()
		y_pred = y_pred[:,select_is]
		prec_pred = select_from_matrix_stack(prec_pred, select_is)
		training_population_mean = training_population_mean[select_is]
		population_mean = population_mean[select_is]
		training_population_cov = training_population_cov[select_is][:,select_is]
		population_cov = population_cov[select_is][:,select_is]

		n_params = len(params)

		true_hyperparameters = np.concatenate([
			population_mean,
			bb.cov_to_std(population_cov)[0]])
		if log_sigma:
			true_hyperparameters[n_params:] = np.log(
				true_hyperparameters[n_params:])

		# A uniform hyperprior, mainly to force some parameters positive.
		# (For MAP/MLE even this is not needed.)
		# Remember sigma_sub can be negative -- it will be treated as zero.
		positive_is = np.asarray(
			[params.index(bb.PALTAS_NAMES[p])
			 for p in ['theta_E', 'gamma']])

		# jit because this will be called from inside jitted functions
		# TODO: make staticmethod
		@numba.njit
		def log_hyperprior(hyperparameters):
			if hyperparameters[positive_is].min() < 0:
				return -np.inf
			# The hyperparameters we get are always log(sigma),
			# since paltas always uses log(sigma).
			log_sigmas = hyperparameters[n_params:]
			if log_sigmas.min() < -15:
				return -np.inf
			return 0

		# Initialize the posterior and give it the network predictions.
		prob_class = (
			paltas.Analysis.hierarchical_inference.ProbabilityClassAnalytical(
				training_population_mean,
				training_population_cov,
				log_hyperprior))
		prob_class.set_predictions(
			mu_pred_array_input=y_pred,
			prec_pred_array_input=prec_pred)

		# Store attributes we need later
		self.prob_class = prob_class
		self.log_sigma = log_sigma
		self.params = params
		self.n_params = n_params
		self.true_hyperparameters = true_hyperparameters
		self.positive_is = positive_is

	def log_posterior(self, x):
		if self.log_sigma:
			# Paltas' posterior always takes log(sigma) parameters,
			# so the hyperprior will be uniform in log(sigma) ...
			return self.prob_class.log_post_omega(x)
		else:
			# Don't want to modify input in-place
			x = x.copy()
			# We got sigmas, but paltas expects log sigmas
			sigmas = x[self.n_params:]
			if sigmas.min() <= 0:
				# No need to ask Paltas, impossible
				return -np.inf
			x[self.n_params:] = np.log(sigmas)
			return self.prob_class.log_post_omega(x)

	def _summary_df(self):
		sigma_prefix = 'log_std' if self.log_sigma else 'std'
		return pd.DataFrame(
			dict(param=(
					['mean_' + p for p in self.params]
					+ [sigma_prefix + '_' + p for p in self.params]
				 ),
				 truth=self.true_hyperparameters))

	def frequentist_asymptotic(
			self,
			use_bounds=False,
			hessian_step=1e-4,
			hessian_method='central',
			**kwargs):
		"""Returns the maximum likelihood estimate and covariance matrix
		for asymptotic frequentist confidence intervals.

		Arguments:
		 - use_bounds: Whether to pass bounds to inference. Defaults to False
		 	to be more robust to non-Gaussianity (e.g. in std_sigma_sub).
		 - hessian_step: step size to use in Hessian computation.
		 - hessian_method: method to use in Hessian computation;
			see numdifftools for details.
		Other arguments will be passed to scipy.minimize.

		Returns tuple of:
		 - DataFrame with results
		 - estimated covariance matrix

		The covariance matrix is estimated from the Hessian of the -2 log
		likelihood; the Hessian is estimated using finite-difference methods.
		"""
		try:
			import numdifftools
		except ImportError:
			raise ImportError("Install numdifftools for frequentist inference")
		if self.log_sigma:
			warnings.warn("Not using a uniform prior, good luck...")

		# Bounds (0, inf) for positive parameters and the sigmas
		if use_bounds:
			bounds = [
				(0, None) if i in self.positive_is
				else (None, None)
				for i in range(self.n_params)]
			bounds += [(np.exp(-14), None)] * self.n_params
		else:
			bounds = None

		# Find the maximum a posteriori / maximum likelihood estimate
		# (they agree for a uniform prior)
		# .. or at least a local max close to the truth.
		def objective(x):
			return -2*self.log_posterior(x)
		with warnings.catch_warnings():
			warnings.filterwarnings(
				"ignore",
				message='invalid value encountered in subtract'
			)
			optresult = minimize(
				objective,
				x0=self.true_hyperparameters,
				bounds=bounds,
				**kwargs,
			)

		# Estimate covariance using the inverse Hessian
		# the minimizers's Hessian inverse estimate (optresult.hess_inv) is
		# not reliable enough, so do a new calculation with numdifftools
		hess = numdifftools.Hessian(
			objective,
			base_step=hessian_step,
			method=hessian_method)(optresult.x)
		cov = np.linalg.inv(hess)

		summary = self._summary_df()
		summary['fit'] = optresult.x
		summary['fit_unc'] = bb.cov_to_std(cov)[0]

		return summary, cov

	def bayesian_mcmc(
			self,
			initial_walker_scatter=1e-3,
			n_samples=int(1e4),
			n_burnin=int(1e3),
			n_walkers=40,
			**kwargs):
		"""Return MCMC inference results

		Arguments:
		 - initial_walker_scatter: amplitude with which to vary walker
			positions. Multiplied by an (n_walkers, n_hyperparams) vector.
		 - n_samples: Number of MCMC samples to use (excluding burn-in)
		 - n_burnin: Number of burn-in samples to use
		 - n_walkers: Number of walkers to use
		Other arguments passed to emcee.EnsembleSampler

		Returns tuple with:
		 - DataFrame with summary of results
		 - chain excluding burn-in, (n_samples, n_hyperparams) array
		"""
		import emcee

		ndim = len(self.true_hyperparameters)

		# Scatter the initial walker states around the true values
		cur_state = (
			self.true_hyperparameters
			+ initial_walker_scatter * np.random.randn(n_walkers, ndim))
		if not self.log_sigma:
			# Don't start at negative sigmas: reflect initial state in 0
			cur_state_sigmas = cur_state[:,self.n_params:]
			cur_state[:,self.n_params:] = np.where(
				cur_state_sigmas < 0,
				-cur_state_sigmas,
				cur_state_sigmas)

		sampler = emcee.EnsembleSampler(
			n_walkers,
			ndim,
			self.log_posterior,
			**kwargs)
		sampler.run_mcmc(cur_state, n_burnin + n_samples, progress=True)
		chain = sampler.chain[:,n_burnin:,:].reshape((-1,ndim))

		summary = self._summary_df()
		summary['fit'] = chain.mean(axis=0)
		summary['fit_unc'] = chain.std(axis=0)

		return summary, chain


def select_from_matrix_stack(matrix_stack, select_i):
	"""Select specific simultaneous row and column indices
	from a stack of matrices"""
	sel_x, sel_y = np.meshgrid(select_i, select_i, indexing='ij')
	return (
		matrix_stack[:, sel_x.ravel(), sel_y.ravel()]
		.reshape([-1] + list(sel_x.shape)))
